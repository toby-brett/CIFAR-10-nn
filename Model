import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch
from dataLoader import *

torch.manual_seed(42)

device = torch.device('cuda')

class ImprovedCNN(nn.Module):
    def __init__(self):
        super().__init__()

        torch.manual_seed(42)

        # Convolutional layers with BatchNorm and increased filters
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Output 32 channels
        self.bn1 = nn.BatchNorm2d(32) # 32 refers to the number of input channels needed to be normalised
        self.conv1_drop = nn.Dropout2d(0.0) # 0.3 = chance of dropout occuring

        # conv 1 takes 3 input channels (each rgb of 1 32x32 image), and outputs 32 feature maps total
        # this gives a total of 32 feature maps as output
        # padding = 1 adds a border of 1 pixel around the image, to prevent large reduction in output size
        # this means that each feature map will have dimensions (32, 32)
        # batch normalisation increases training speed - https://www.youtube.com/watch?v=DtEq44FTPM4
        # batch normalisation also decreases the affect of starting weights and biases
        # batch normalisation acts as a regularizer, to prevent overtraining


        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Input 32 channels, output 64
        self.bn2 = nn.BatchNorm2d(64)
        self.conv2_drop = nn.Dropout2d(0.0)

        # 32 inputs of size (32x32), 64 feature maps of size (32x32) (due to padding)

        self.conv3 = nn.Conv2d(64, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)

        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(512)

        #512 feature maps of size (32x32)

        # Adaptive pooling to ensure compatibility with FC layer input
        self.pool = nn.AdaptiveAvgPool2d((4, 4))

        # each feature map is pooled to size

        # Fully connected layers with increased neurons
        self.fcl = nn.Linear(512 * 4 * 4, 256) # 128 = feature maps, 6 = rows, 6 = columns
        self.fcl2 = nn.Linear(256, 256)
        self.fcl3 = nn.Linear(256, 10)

    def forward(self, x):
        
        x = F.relu(self.bn1(self.conv1(x)))  # First Conv layer, output 32 channels
        x = self.conv1_drop(F.max_pool2d(x, 2))  # Apply pooling

        x = F.relu(self.bn2(self.conv2(x)))  # Second Conv layer, output 64 channels
        x = self.conv2_drop(F.max_pool2d(x, 2))  # Apply pooling

        x = F.relu(self.bn3(self.conv3(x)))  # Third Conv layer, output 128 channels

        x = F.relu(self.bn4(self.conv4(x)))
        x = self.pool(x)  # Adaptive pooling

        # Flatten for fully connected layers
        x = x.view(x.size(0), -1)  # Flatten the output to match the input of the FC layers
        x = F.relu(self.fcl(x))  # First fully connected layer
        x = F.dropout(x, 0.1, training=self.training)  # Dropout

        x = F.relu(self.fcl2(x))  # Second fully connected layer
        x = self.fcl3(x)  # Output layer

        return F.softmax(x, dim=1)


Model = ImprovedCNN()
Model = Model.to(device)
Model.load_state_dict(torch.load(f='/Users/tobybrett/Desktop/Deep learning/Models/Image recognition/CIFAR-10 model (78 percent)'))

from PIL import Image

image = Image.open('/Users/tobybrett/Desktop/Deep learning/Models/Image recognition')

image.show()
