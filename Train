import torch
from main import *

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ImprovedCNN()
model = model.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.0005)
loss_fn = nn.CrossEntropyLoss()

def train(epoch):
  model.train()
  for batch_index, (data, target) in enumerate(loaders['train']): # loads each data batch
    data, target = data.to(device), target.to(device)
    print(data)
    preds = model(data)
    loss = loss_fn(preds, target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if batch_index % 20 == 0:
      print(f'Train Epoch: {epoch} [{batch_index * len(data)}/{len(loaders["train"].dataset)} ({100. * batch_index / len(loaders["train"]):.0f}%)]\t{loss.item():.6f}')

def test():
  model.eval()
  test_loss = 0
  correct = 0

  with torch.inference_mode():
    for data, target in loaders['test']:
      data, target = data.to(device), target.to(device)  # Move data and target to device
      output = model(data)  # Get model predictions
      test_loss += loss_fn(output, target).item()  # Convert tensor loss to scalar using .item()
      pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max value in each prediction
      correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions

  test_loss /= len(loaders['test'].dataset)
  print(f'\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders["test"].dataset)} ({100. * correct / len(loaders["test"].dataset):.0f}%)\n')


for epoch in range(1, 21):
  train(epoch)
  test()
